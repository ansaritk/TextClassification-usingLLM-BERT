# -*- coding: utf-8 -*-
"""
Automatically generated by Colab.

# Text classification - IMDB Dataset
"""


"""## Get Dataset"""

from datasets import load_dataset

imdb = load_dataset("imdb")
imdb

"""- Similar to a python dictionary, where each key corresponds to a different split"""

imdb['train'][0]

imdb['test'][:3]

imdb['train'] = imdb['train'].shuffle(seed=1).select(range(2000))
imdb['train']

imdb_train_validation = imdb['train'].train_test_split(train_size=0.8)
imdb_train_validation

# Rename the 'test' split to 'validation' by creating a new DatasetDict
imdb_train_validation = {
    'train': imdb_train_validation['train'],
    'validation': imdb_train_validation['test']
}
imdb_train_validation

imdb.update(imdb_train_validation)
imdb

imdb['test'] = imdb['test'].shuffle(seed=1).select(range(400))
imdb['test']

imdb['unsupervised'][:3]

imdb.pop('unsupervised')
imdb

"""## Overview of IMDB Dataset"""

import pandas as pd
import matplotlib.pyplot as plt
pd.set_option('max_colwidth', 250)

imdb.set_format('pandas')
df = imdb['train'][:]
df.sample(frac=1 ,random_state=1).head(10)

df.loc[0, 'text']

df['text'] = df.text.str.replace('<br />', '')
df.loc[0, 'text']

df.label.value_counts()

df["Words per review"] = df["text"].str.split().apply(len)
df.boxplot("Words per review", by="label", grid=False, showfliers=False,
           color="black")
plt.suptitle("")
plt.xlabel("")
plt.show()

# 0 is negative
# 1 is positive
df[df.text.str.len() < 200]

imdb.reset_format()

"""## Tokenizer"""

from transformers import AutoTokenizer

#checkpoint = "distilbert-base-cased"
checkpoint = "bert-base-cased"
tokenizer = AutoTokenizer.from_pretrained(checkpoint)

def tokenize_function(batch):
    return tokenizer(batch["text"], padding=True, truncation=True)

imdb_encoded = imdb.map(tokenize_function, batched=True, batch_size=None)
imdb_encoded

print(imdb_encoded['train'][0])

"""## Tiny IMDB"""

import transformers
import re

[x for x in dir(transformers) if re.search(r'^AutoModel', x)]

import torch
from transformers import AutoModelForSequenceClassification

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
num_labels = 2
model = (AutoModelForSequenceClassification
         .from_pretrained(checkpoint, num_labels=num_labels)
         .to(device))
model

from datasets import DatasetDict

tiny_imdb = DatasetDict()
tiny_imdb['train'] = imdb['train'].shuffle(seed=1).select(range(50))
tiny_imdb['validation'] = imdb['validation'].shuffle(seed=1).select(range(10))
tiny_imdb['test'] = imdb['test'].shuffle(seed=1).select(range(10))

tiny_imdb_encoded = tiny_imdb.map(tokenize_function, batched=True, batch_size=None)
tiny_imdb_encoded

from transformers import Trainer, TrainingArguments

batch_size = 8
logging_steps = len(tiny_imdb_encoded["train"]) // batch_size
model_name = f"{checkpoint}-finetuned-tiny-imdb"
training_args = TrainingArguments(output_dir=model_name,
                                  report_to="none",
                                  num_train_epochs=2,
                                  learning_rate=2e-5,
                                  per_device_train_batch_size=batch_size,
                                  per_device_eval_batch_size=batch_size,
                                  weight_decay=0.01,
                                  evaluation_strategy="epoch",
                                  disable_tqdm=False,
                                  logging_steps=logging_steps,
                                  log_level="error",
                                  optim='adamw_torch'
                                  )
training_args

from transformers import Trainer

torch.cuda.empty_cache()

trainer = Trainer(model=model,
                  args=training_args,
                  train_dataset=tiny_imdb_encoded["train"],
                  eval_dataset=tiny_imdb_encoded["validation"],
                  tokenizer=tokenizer)
trainer.train();

preds = trainer.predict(tiny_imdb_encoded['test'])
preds

preds.predictions.shape

preds.predictions.argmax(axis=-1)

preds.label_ids

from sklearn.metrics import accuracy_score

accuracy_score(preds.label_ids, preds.predictions.argmax(axis=-1))

def get_accuracy(preds):
  predictions = preds.predictions.argmax(axis=-1)
  labels = preds.label_ids
  accuracy = accuracy_score(preds.label_ids, preds.predictions.argmax(axis=-1))
  return {'accuracy': accuracy}

from transformers import Trainer

torch.cuda.empty_cache()

trainer = Trainer(model=model,
                  compute_metrics=get_accuracy,
                  args=training_args,
                  train_dataset=tiny_imdb_encoded["train"],
                  eval_dataset=tiny_imdb_encoded["validation"],
                  tokenizer=tokenizer)
trainer.train();

"""## Training run"""

batch_size = 8
logging_steps = len(imdb_encoded["train"]) // batch_size
model_name = f"{checkpoint}-finetuned-imdb"
training_args = TrainingArguments(output_dir=model_name,
                                  report_to="none",
                                  num_train_epochs=2,
                                  learning_rate=2e-5,
                                  per_device_train_batch_size=batch_size,
                                  per_device_eval_batch_size=batch_size,
                                  weight_decay=0.01,
                                  eval_strategy="epoch",
                                  disable_tqdm=False,
                                  logging_steps=logging_steps,
                                  log_level="error",
                                  optim='adamw_torch'
                                  )

from transformers import Trainer

torch.cuda.empty_cache()

trainer = Trainer(model=model,
                  args=training_args,
                  compute_metrics=get_accuracy,
                  train_dataset=imdb_encoded["train"],
                  eval_dataset=imdb_encoded["validation"],
                  tokenizer=tokenizer)
trainer.train();

trainer.evaluate()

trainer.save_model()

model_name


"""## Real-time application"""

from transformers import pipeline
classifier = pipeline('text-classification', model=model_name)
classifier('This is not my idea of fun')

classifier('This was beyond incredible')